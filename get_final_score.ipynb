{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install gym[atari]\n",
    "# !pip uninstall ale-py\n",
    "# !pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config.config2_ddqn import config as config2\n",
    "from Config.config4_ddqn import config as config4\n",
    "\n",
    "from Config.dueling_config1 import config as dueling_config1\n",
    "from Config.dueling_config2 import config as dueling_config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_lives = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        \n",
    "        reward = 0\n",
    "        t = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQNAgent import DQNAgent\n",
    "import DuelingDQNAgent\n",
    "from Environment import PreprocessAtariObs, make_env\n",
    "\n",
    "_env = make_env(seed=2356, skip=4)\n",
    "state_shape = _env.observation_space.shape\n",
    "n_actions = _env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dqn_exp():\n",
    "    checkpoint = torch.load(\"best_score_\" + config2['file_name'] , map_location=torch.device(device))\n",
    "    agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)\n",
    "\n",
    "def eval_dqn_pr():\n",
    "    checkpoint = torch.load(\"best_score_\" + config4['file_name'] , map_location=torch.device(device))\n",
    "    agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)\n",
    "\n",
    "def eval_dueling_dqn_exp():\n",
    "    checkpoint = torch.load(\"best_score_\" + dueling_config2['file_name'] , map_location=torch.device(device))\n",
    "    agent = DuelingDQNAgent.DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)\n",
    "\n",
    "def eval_dueling_dqn_pr():\n",
    "    checkpoint = torch.load(\"best_score_\" + dueling_config1['file_name'] , map_location=torch.device(device))\n",
    "    agent = DuelingDQNAgent.DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "    agent.network.load_state_dict(checkpoint['network'])\n",
    "    agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "    env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "    return evaluate(env, agent, n_games=n_lives, greedy=True, t_max=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_size 1024\n",
      "fc_size 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinik\\Githubik\\ULTRA_SUPER_MEGA_POWER_v2_zxc\\DQNAgent.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  states = torch.tensor(states, device=model_device, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n",
      "fc_size 1024\n"
     ]
    }
   ],
   "source": [
    "dqn_pr = np.asarray([eval_dqn_pr() for i in range(10)])\n",
    "dueling_dqn_pr = np.asarray([eval_dueling_dqn_pr() for i in range(10)])\n",
    "dqn_exp = np.asarray([eval_dqn_exp() for i in range(10)])\n",
    "dueling_dqn_exp = np.asarray([eval_dueling_dqn_exp() for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_dqn_pr : 5226.3\n",
      "mean_dueling_dqn_pr : 3559.8\n",
      "mean_dqn_exp : 5680.1\n",
      "mean_dueling_dqn_exp : 3965.0\n"
     ]
    }
   ],
   "source": [
    "print(\"mean_dqn_pr :\", np.mean(dqn_pr))\n",
    "print(\"mean_dueling_dqn_pr :\", np.mean(dueling_dqn_pr))\n",
    "print(\"mean_dqn_exp :\", np.mean(dqn_exp))\n",
    "print(\"mean_dueling_dqn_exp :\", np.mean(dueling_dqn_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_dqn_pr : 8276.0\n",
      "max_dueling_dqn_pr : 5436.0\n",
      "max_dqn_exp : 6723.0\n",
      "max_dueling_dqn_exp : 7187.0\n"
     ]
    }
   ],
   "source": [
    "print(\"max_dqn_pr :\", np.max(dqn_pr))\n",
    "print(\"max_dueling_dqn_pr :\", np.max(dueling_dqn_pr))\n",
    "print(\"max_dqn_exp :\", np.max(dqn_exp))\n",
    "print(\"max_dueling_dqn_exp :\", np.max(dueling_dqn_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"best_score_\" + config2['file_name'] , map_location=torch.device(device))\n",
    "# agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "# agent.network.load_state_dict(checkpoint['network'])\n",
    "# agent.target_network.load_state_dict(checkpoint['target_network'])\n",
    "# env = make_env(skip=4, clip_rewards=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# evaluate(env, agent, n_games=n_lives, greedy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
